import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import anndata
import geopandas as gpd
import scanpy as sc
from PIL import Image # Converting image
import os # for parquet file
from pathlib import Path

# from tifffile import imread, imwrite
# from csbdeep.utils import normalize
# from stardist.models import StarDist2D
from shapely.geometry import Polygon, Point
from scipy import sparse
from matplotlib.colors import ListedColormap
from matplotlib.widgets import LassoSelector
from matplotlib.path import Path as MplPath 
%matplotlib widget


SEGMENTATION_PATH = Path("/mnt/c/Users/jonan/Documents/1Work/RoseLab/Spatial/"
                         "dietary_droject/data/cell_segmentation")

sample_id = "F07833"

adata_path = SEGMENTATION_PATH / sample_id / f"{sample_id}_grouped_filtered_adata_rebinned.h5ad"
gdf_file = SEGMENTATION_PATH / sample_id / f"{sample_id}_gdf_rebinned.gpkg"

# Load adata
ST_sample = sc.read_h5ad(adata_path)
# load your polygons
geo_file = gpd.read_file(gdf_file)


print(ST_sample.obs.columns) # shows columns in observation metadata (cells/spots)
print(ST_sample.var.columns) # shows columns in variable metadata (genes)


print(geo_file.columns)


ST_sample.obs['id']


# 1. Grab ID strings
id_strings = ST_sample.obs['id']

# 2. Strip "ID_" and convert to integers
id_numbers = id_strings.str.replace("ID_", "").astype(int)

# 3. Compute summary statistics
id_summary = {
    "n_ids": len(id_numbers),
    "id_min": np.min(id_numbers),
    "id_q1": np.percentile(id_numbers, 25),
    "id_median": np.median(id_numbers),
    "id_mean": np.mean(id_numbers),
    "id_q3": np.percentile(id_numbers, 75),
    "id_max": np.max(id_numbers),
    "id_std": np.std(id_numbers),
    "id_iqr": np.percentile(id_numbers, 75) - np.percentile(id_numbers, 25)
}

# 4. Show nicely
summary_df = pd.DataFrame([id_summary])
print(summary_df.T)



id_numbers.mode()



ST_sample.obs['id'].value_counts()



ST_sample.var.head()


ST_sample.uns.keys()  # unstructured data (e.g., clustering results, metadata)


ST_sample.obsm.keys() # multidimensional annotations (e.g., PCA, UMAP coordinates)


ST_sample.varm.keys()








SEGMENTATION_PATH = Path("/mnt/c/Users/jonan/Documents/1Work/RoseLab/Spatial/"
                         "dietary_droject/data/cell_segmentation")

sample_id = "F07833"

# adata_path = SEGMENTATION_PATH / sample_id / f"{sample_id}_grouped_filtered_adata_rebinned.h5ad"
gdf_file = SEGMENTATION_PATH / sample_id / f"{sample_id}_gdf_rebinned.gpkg"

# Load adata
# ST_sample = sc.read_h5ad(adata_path)
# load your polygons
geo_file = gpd.read_file(gdf_file)














# Checking the ID matching
import pandas as pd

SEGMENTATION_PATH = Path("/mnt/c/Users/jonan/Documents/1Work/RoseLab/Spatial/"
                         "dietary_droject/data/cell_segmentation")

sample_id = "F07833"

# Load your AnnData (filtered and rebinned version)
import scanpy as sc
ST_sample = sc.read_h5ad(SEGMENTATION_PATH / sample_id /f"{sample_id}_grouped_filtered_adata_rebinned.h5ad")
# Load your saved barcode-to-nucleus mapping
barcode_to_nucleus = pd.read_csv( SEGMENTATION_PATH / sample_id / f"{sample_id}_barcode_to_nucleus_mapping.csv")


# Extract IDs from ST_sample
ids_from_adata = set(ST_sample.obs['id'])

# Extract IDs from CSV
ids_from_csv = set(barcode_to_nucleus['id'])

# sanity checks:
missing_in_csv = ids_from_adata - ids_from_csv
missing_in_adata = ids_from_csv - ids_from_adata

print(f"Number of IDs in ST_sample: {len(ids_from_adata)}")
print(f"Number of IDs in CSV: {len(ids_from_csv)}")

if len(missing_in_csv) == 0 and len(missing_in_adata) == 0:
    print("✅ Perfect match: All IDs match between ST_sample and the CSV!")
else:
    print("❌ Mismatch detected:")
    if missing_in_csv:
        print(f" - IDs in ST_sample but missing from CSV: {missing_in_csv}")
    if missing_in_adata:
        print(f" - IDs in CSV but missing from ST_sample: {missing_in_adata}")






